{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb757479-7611-42b5-98cb-5ac94e3e94a6",
   "metadata": {},
   "source": [
    "### 使用Bayesian模型处理垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae134fad-8155-45de-9800-81f23b7e7a73",
   "metadata": {},
   "source": [
    "#### 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1072aa69-0dc1-4bee-9c4c-afb3b0645e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) # 空集合，存储最终的词汇表\n",
    "    for data in dataSet:\n",
    "        # vocabSet = vocabSet.union(set([str.lower(x) for x in data]))\n",
    "        vocabSet = vocabSet | set([str.lower(x) for x in data])\n",
    "    return sorted(list(vocabSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00bd2358-c377-4dc1-8550-9f10c7189adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'he', 'i', 'love', 'you']\n"
     ]
    }
   ],
   "source": [
    "dataSet = [['a','i','love','You'],\n",
    "           ['he','Love','you']]\n",
    "\n",
    "vocabList = createVocabList(dataSet)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19aec0-5c82-42d1-9c78-4c0d8421a415",
   "metadata": {},
   "source": [
    "##### 词集模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959a35d7-9eae-4e66-a63f-3eeb129a5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList) # 初始化向量，长度与词汇表长度一致\n",
    "    LowerInputSet = [str.lower(x) for x in inputSet]\n",
    "    for i in range(len(vocabList)):\n",
    "        if vocabList[i] in LowerInputSet:\n",
    "            returnVec[i] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f96a3c8-5bf8-4c8f-a173-126ce65560c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(setOfWords2Vec(vocabList,['I','a','you']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdcf31-ec8c-441e-afe3-c5845f5b2226",
   "metadata": {},
   "source": [
    "##### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee96f0d-4c65-4c41-b107-fd1f5756389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList) # 初始化向量，长度与词汇表长度一致\n",
    "    LowerInputSet = [str.lower(x) for x in inputSet]\n",
    "    for word in LowerInputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dca55f5-a26e-4fb1-9acb-794e4e9f1d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(bagOfWords2Vec(vocabList,['I','I','a','you']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05c6bc-3125-41cc-8ea3-8c1e3f14dd28",
   "metadata": {},
   "source": [
    "##### 处理邮件内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13383d9-16b7-45a5-a596-06d520c68739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W+', bigString) # 返回一个包含分割后单词的列表，每个单词由连续的字母/数字组成，且单词间由非单词字符分隔‌\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c3dd4c-0ca6-4308-a5ff-665daabb06e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'you', '30kg', '20ml']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textParse('I love You 30Kg --20ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7ef65-a793-4cd5-8024-f1b6628ab232",
   "metadata": {},
   "source": [
    "##### 加载数据并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6a58c2-4f8b-4af5-8168-b19c9a30206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata():\n",
    "    docList = []\n",
    "    classList = []\n",
    "\n",
    "    num = 26 # 垃圾邮件和非垃圾邮件各有25封\n",
    "\n",
    "    for i in range(1,num):\n",
    "        if i==17: # 第17封垃圾邮件编码为windows-1252\n",
    "            wordList = textParse(open('data/email/spam/%d.txt' % i,encoding='Windows-1252').read())\n",
    "        else:\n",
    "            wordList = textParse(open('data/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "\n",
    "        if i==6: # 第6封非垃圾邮件编码为windows-1252\n",
    "            wordList = textParse(open('data/email/ham/%d.txt' % i,encoding='Windows-1252').read())\n",
    "        else:\n",
    "            wordList = textParse(open('data/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(0)\n",
    "\n",
    "    vocabList = createVocabList(docList)\n",
    "    X = []\n",
    "    for doc in docList:\n",
    "        X.append(setOfWords2Vec(vocabList,doc))\n",
    "    return X, classList, vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a75199-4c3a-414d-912d-60a36b269d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.metrics import accuracy_score\n",
    "X,y,vocabList = loaddata()\n",
    "\n",
    "model = nb.MultinomialNB()\n",
    "model.fit(X,y)\n",
    "\n",
    "y_hat = model.predict(X)\n",
    "print(\"正确率：\",accuracy_score(y_hat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a98eb17-adb8-4e80-a179-a1decf728ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'was',\n",
       " 'guy',\n",
       " 'the',\n",
       " 'gas',\n",
       " 'station',\n",
       " 'who',\n",
       " 'told',\n",
       " 'that',\n",
       " 'knew',\n",
       " 'mandarin',\n",
       " 'and',\n",
       " 'python',\n",
       " 'could',\n",
       " 'get',\n",
       " 'job',\n",
       " 'with',\n",
       " 'the',\n",
       " 'fbi']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textParse(open('data/email/ham/%d.txt' % 5).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259dec9f-f53f-44b9-a5a2-6d8a763844c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-1252\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    " \n",
    "with open('data/email/ham/6.txt', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef6e36-b6d6-4639-879d-05c9bb66a420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
